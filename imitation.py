import sys
import argparse
import numpy as np
import keras
import random
import gym
from keras.utils.np_utils import to_categorical
from tqdm import tqdm
import pdb



class Imitation():
    def __init__(self, args, model_config_path, expert_weights_path):
        # Load the expert model.
        with open(model_config_path, 'r') as f:
            self.expert = keras.models.model_from_json(f.read())
        self.expert.load_weights(expert_weights_path)
        
        # Initialize the cloned model (to be trained).
        with open(model_config_path, 'r') as f:
            self.model = keras.models.model_from_json(f.read())

        # TODO: Define any training operations and optimizers here, initialize
        #       your variables, or alternatively compile your model here.
        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
        self.args = args

    def run_expert(self, env, render=False):
        # Generates an episode by running the expert policy on the given env.
        return Imitation.generate_episode_expert(self.args, self.expert, env, render)

    def run_model(self, env, render=False):
        # Generates an episode by running the cloned policy on the given env.
        return Imitation.generate_episode_model(self.args, self.model, env, render)

    @staticmethod
    def generate_episode_expert(args, model, env, render=False):
        # Generates an episode by running the given model on the given env.
        # Returns:
        # - a list of states, indexed by time step
        # - a list of actions, indexed by time step
        # - a list of rewards, indexed by time step
        # TODO: Implement this method.
        states = []
        actions = []
        rewards = []

        #Run an episode using the model
        obs = env.reset()
        done = False
        step = 0
        while not done and step <= args.max_steps:
            if args.render:
                env.render()
            states.append(obs)
            obs = obs.reshape(1, obs.shape[0])
            next_act_probs = model.predict(obs)
            next_act = np.argmax(next_act_probs)    #Pick most likely action
            categorical_labels = to_categorical(next_act, num_classes=env.action_space.n)   #COnverts to one hot outputs
            actions.append(categorical_labels)          #For expert action chosen is stored
            next_obs, reward, done, _ = env.step(next_act)
            rewards.append(reward)
            obs = next_obs
            step += 1

        return np.array(states), np.array(actions), np.array(rewards)

    @staticmethod
    def generate_episode_model(args, model, env, render=False):
        # Generates an episode by running the given model on the given env.
        # Returns:
        # - a list of states, indexed by time step
        # - a list of actions, indexed by time step
        # - a list of rewards, indexed by time step
        # TODO: Implement this method.
        states = []
        actions = []
        rewards = []

        #Run an episode using the model
        obs = env.reset()
        done = False
        step = 0
        while not done and step <= args.max_steps:
            if args.render:
                env.render()
            states.append(obs)
            obs = obs.reshape(1, obs.shape[0])
            next_act_probs = model.predict(obs)
            next_act = np.argmax(next_act_probs)    #Pick most likely action
            actions.append(next_act_probs)          #For model probabilities need to be stored
            next_obs, reward, done, _ = env.step(next_act)
            rewards.append(reward)
            obs = next_obs
            step += 1

        return states, actions, rewards
    
    def train(self, env, num_episodes=10, num_epochs=50, render=False):
        # Trains the model on training data generated by the expert policy.
        # Args:
        # - env: The environment to run the expert policy on. 
        # - num_episodes: # episodes to be generated by the expert.
        # - num_epochs: # epochs to train on the data generated by the expert.
        # - render: Whether to render the environment.
        # Returns the final loss and accuracy.
        # TODO: Implement this method. It may be helpful to call the class
        #       method run_expert() to generate training data.

        args = self.args
        #Construct the training dataset
        print("Generating training data by running expert policy")

        for i in range(num_episodes):
            states, actions, rewards = self.run_expert(env)
            if i == 0:
                train_states = states
                train_actions = actions
                train_rewards = rewards
            else:
                train_states = np.concatenate((train_states, states), axis=0)        
                train_actions = np.concatenate((train_actions, actions), axis=0)
                train_rewards = np.concatenate((train_rewards, rewards), axis=0) 
        
        print("Train data has been generated using the expert policy")

        #Train the model using data from the expert
        self.model.fit(train_states, train_actions, epochs=num_epochs, batch_size=args.batch_size)

        #Evaluate the model
        # evaluate the model
        scores = self.model.evaluate(train_states, train_actions)
        #print("\n%s: %.2f%%" % (imitater.model.metrics_names[1], scores[1]*100))

        loss = scores[0]
        acc = scores[1]*100
        self.model.save_weights("imitater_" + str(num_episodes)+".h5")

        return loss, acc

    def test(self, env,  num_of_episodes = 100):   
        trained_weights_path = "imitater_" + str(self.args.episodes)+".h5"
        tot_rewards = []
        self.model.load_weights(trained_weights_path)
        for i in range(num_of_episodes):
            _, _, rewards = self.run_model(env) 
            tot_rewards.append(np.sum(rewards))

        return np.mean(tot_rewards), np.std(tot_rewards)

    def test_expert(self, env,  num_of_episodes = 100):   
        tot_rewards = []
        for i in range(num_of_episodes):
            _, _, rewards = self.run_expert(env) 
            tot_rewards.append(np.sum(rewards))

        return np.mean(tot_rewards), np.std(tot_rewards)


def parse_arguments():
    # Command-line flags are defined here.
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-config-path', dest='model_config_path',
                        type=str, default='LunarLander-v2-config.json',
                        help="Path to the model config file.")
    parser.add_argument('--expert-weights-path', dest='expert_weights_path',
                        type=str, default='LunarLander-v2-weights.h5',
                        help="Path to the expert weights file.")

    # https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse
    parser_group = parser.add_mutually_exclusive_group(required=False)
    parser_group.add_argument('--render', dest='render',
                              action='store_true',
                              help="Whether to render the environment.")
    parser_group.add_argument('--no-render', dest='render',
                              action='store_false',
                              help="Whether to render the environment.")
    parser.set_defaults(render=False)
    
    parser_group.add_argument('--test', dest='test',
                              action='store_true',
                              help="Whether to run in test mode")
    parser_group.add_argument('--train', dest='test',
                              action='store_false',
                              help="Whether to run in train mode")
    parser.set_defaults(test=False)
    
    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',
                    help='learning rate')
    parser.add_argument('--episodes', type=int, default=100, metavar='N',
                    help='Number of episodes to generate from expert')
    parser.add_argument('--batch_size', type=int, default=32, metavar='N',
                    help='Batch Size')
    parser.add_argument('--max_steps', type=int, default=1000, metavar='N',
                    help='Maximum length of running an episode')
    parser.add_argument('--epochs', type=int, default=50, metavar='N',
                    help='Number of epochs to train the model for')
    return parser.parse_args()


def main(args):
    # Parse command-line arguments.
    args = parse_arguments()
    model_config_path = args.model_config_path
    expert_weights_path = args.expert_weights_path
    render = args.render
    
    # Create the environment.
    env = gym.make('LunarLander-v2')
    
    #Create Instance of imitation class
    imitater = Imitation(args, args.model_config_path, args.expert_weights_path)
    if not args.test:
        loss, acc = imitater.train(env, args.episodes, args.epochs, args.render)
        #imitater.save_weights("imitater_" + str(args.episodes)+".h5")
        print("The imitater model has been trained")
        print("Accuracy is: "+str(acc) + " and Average Train loss is: "+str(loss))
        # TODO: Train cloned models using imitation learning, and record their
        #       performance.

    else:
        mean, std = imitater.test(env)
        print("Number of training episodes: " +str(args.episodes) + " \nMean reward is: " + str(mean) + " and Std of rewards is: "+str(std)) 
        #mean, std = imitater.test_expert(env)
        #print("Testing Expert Model: Mean reward is: " + str(mean) + " and Std of rewards is: "+str(std))

if __name__ == '__main__':
  main(sys.argv)

